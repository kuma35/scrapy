# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008–2021, Scrapy developers
# This file is distributed under the same license as the Scrapy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Scrapy 2.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-10 01:26+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../faq.rst:4
msgid "Frequently Asked Questions"
msgstr ""

#: ../../faq.rst:9
msgid "How does Scrapy compare to BeautifulSoup or lxml?"
msgstr ""

#: ../../faq.rst:11
msgid ""
"`BeautifulSoup`_ and `lxml`_ are libraries for parsing HTML and XML. "
"Scrapy is an application framework for writing web spiders that crawl web"
" sites and extract data from them."
msgstr ""

#: ../../faq.rst:15
msgid ""
"Scrapy provides a built-in mechanism for extracting data (called "
":ref:`selectors <topics-selectors>`) but you can easily use "
"`BeautifulSoup`_ (or `lxml`_) instead, if you feel more comfortable "
"working with them. After all, they're just parsing libraries which can be"
" imported and used from any Python code."
msgstr ""

#: ../../faq.rst:21
msgid ""
"In other words, comparing `BeautifulSoup`_ (or `lxml`_) to Scrapy is like"
" comparing `jinja2`_ to `Django`_."
msgstr ""

#: ../../faq.rst:30
msgid "Can I use Scrapy with BeautifulSoup?"
msgstr ""

#: ../../faq.rst:32
msgid ""
"Yes, you can. As mentioned :ref:`above <faq-scrapy-bs-cmp>`, "
"`BeautifulSoup`_ can be used for parsing HTML responses in Scrapy "
"callbacks. You just have to feed the response's body into a "
"``BeautifulSoup`` object and extract whatever data you need from it."
msgstr ""

#: ../../faq.rst:38
msgid ""
"Here's an example spider using BeautifulSoup API, with ``lxml`` as the "
"HTML parser::"
msgstr ""

#: ../../faq.rst:62
msgid ""
"``BeautifulSoup`` supports several HTML/XML parsers. See `BeautifulSoup's"
" official documentation`_ on which ones are available."
msgstr ""

#: ../../faq.rst:69
msgid "Did Scrapy \"steal\" X from Django?"
msgstr ""

#: ../../faq.rst:71
msgid ""
"Probably, but we don't like that word. We think Django_ is a great open "
"source project and an example to follow, so we've used it as an "
"inspiration for Scrapy."
msgstr ""

#: ../../faq.rst:75
msgid ""
"We believe that, if something is already done well, there's no need to "
"reinvent it. This concept, besides being one of the foundations for open "
"source and free software, not only applies to software but also to "
"documentation, procedures, policies, etc. So, instead of going through "
"each problem ourselves, we choose to copy ideas from those projects that "
"have already solved them properly, and focus on the real problems we need"
" to solve."
msgstr ""

#: ../../faq.rst:82
msgid ""
"We'd be proud if Scrapy serves as an inspiration for other projects. Feel"
" free to steal from us!"
msgstr ""

#: ../../faq.rst:86
msgid "Does Scrapy work with HTTP proxies?"
msgstr ""

#: ../../faq.rst:88
msgid ""
"Yes. Support for HTTP proxies is provided (since Scrapy 0.8) through the "
"HTTP Proxy downloader middleware. See "
":class:`~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware`."
msgstr ""

#: ../../faq.rst:93
msgid "How can I scrape an item with attributes in different pages?"
msgstr ""

#: ../../faq.rst:95
msgid "See :ref:`topics-request-response-ref-request-callback-arguments`."
msgstr ""

#: ../../faq.rst:99
msgid "Scrapy crashes with: ImportError: No module named win32api"
msgstr ""

#: ../../faq.rst:101
msgid "You need to install `pywin32`_ because of `this Twisted bug`_."
msgstr ""

#: ../../faq.rst:107
msgid "How can I simulate a user login in my spider?"
msgstr ""

#: ../../faq.rst:109
msgid "See :ref:`topics-request-response-ref-request-userlogin`."
msgstr ""

#: ../../faq.rst:114
msgid "Does Scrapy crawl in breadth-first or depth-first order?"
msgstr ""

#: ../../faq.rst:116
msgid ""
"By default, Scrapy uses a `LIFO`_ queue for storing pending requests, "
"which basically means that it crawls in `DFO order`_. This order is more "
"convenient in most cases."
msgstr ""

#: ../../faq.rst:120
msgid ""
"If you do want to crawl in true `BFO order`_, you can do it by setting "
"the following settings::"
msgstr ""

#: ../../faq.rst:127
msgid ""
"While pending requests are below the configured values of "
":setting:`CONCURRENT_REQUESTS`, :setting:`CONCURRENT_REQUESTS_PER_DOMAIN`"
" or :setting:`CONCURRENT_REQUESTS_PER_IP`, those requests are sent "
"concurrently. As a result, the first few requests of a crawl rarely "
"follow the desired order. Lowering those settings to ``1`` enforces the "
"desired order, but it significantly slows down the crawl as a whole."
msgstr ""

#: ../../faq.rst:136
msgid "My Scrapy crawler has memory leaks. What can I do?"
msgstr ""

#: ../../faq.rst:138
msgid "See :ref:`topics-leaks`."
msgstr ""

#: ../../faq.rst:140
msgid ""
"Also, Python has a builtin memory leak issue which is described in :ref"
":`topics-leaks-without-leaks`."
msgstr ""

#: ../../faq.rst:144
msgid "How can I make Scrapy consume less memory?"
msgstr ""

#: ../../faq.rst:146
msgid "See previous question."
msgstr ""

#: ../../faq.rst:149
msgid "How can I prevent memory errors due to many allowed domains?"
msgstr ""

#: ../../faq.rst:151
msgid ""
"If you have a spider with a long list of "
":attr:`~scrapy.spiders.Spider.allowed_domains` (e.g. 50,000+), consider "
"replacing the default "
":class:`~scrapy.spidermiddlewares.offsite.OffsiteMiddleware` spider "
"middleware with a :ref:`custom spider middleware <custom-spider-"
"middleware>` that requires less memory. For example:"
msgstr ""

#: ../../faq.rst:158
msgid ""
"If your domain names are similar enough, use your own regular expression "
"instead joining the strings in "
":attr:`~scrapy.spiders.Spider.allowed_domains` into a complex regular "
"expression."
msgstr ""

#: ../../faq.rst:163
msgid ""
"If you can `meet the installation requirements`_, use pyre2_ instead of "
"Python’s re_ to compile your URL-filtering regular expression. See "
":issue:`1908`."
msgstr ""

#: ../../faq.rst:167
msgid "See also other suggestions at `StackOverflow`_."
msgstr ""

#: ../../faq.rst:169
msgid ""
"Remember to disable "
":class:`scrapy.spidermiddlewares.offsite.OffsiteMiddleware` when you "
"enable your custom implementation::"
msgstr ""

#: ../../faq.rst:184
msgid "Can I use Basic HTTP Authentication in my spiders?"
msgstr ""

#: ../../faq.rst:186
msgid ""
"Yes, see "
":class:`~scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware`."
msgstr ""

#: ../../faq.rst:189
msgid "Why does Scrapy download pages in English instead of my native language?"
msgstr ""

#: ../../faq.rst:191
msgid ""
"Try changing the default `Accept-Language`_ request header by overriding "
"the :setting:`DEFAULT_REQUEST_HEADERS` setting."
msgstr ""

#: ../../faq.rst:197
msgid "Where can I find some example Scrapy projects?"
msgstr ""

#: ../../faq.rst:199
msgid "See :ref:`intro-examples`."
msgstr ""

#: ../../faq.rst:202
msgid "Can I run a spider without creating a project?"
msgstr ""

#: ../../faq.rst:204
msgid ""
"Yes. You can use the :command:`runspider` command. For example, if you "
"have a spider written in a ``my_spider.py`` file you can run it with::"
msgstr ""

#: ../../faq.rst:209
msgid "See :command:`runspider` command for more info."
msgstr ""

#: ../../faq.rst:212
msgid "I get \"Filtered offsite request\" messages. How can I fix them?"
msgstr ""

#: ../../faq.rst:214
msgid ""
"Those messages (logged with ``DEBUG`` level) don't necessarily mean there"
" is a problem, so you may not need to fix them."
msgstr ""

#: ../../faq.rst:217
msgid ""
"Those messages are thrown by the Offsite Spider Middleware, which is a "
"spider middleware (enabled by default) whose purpose is to filter out "
"requests to domains outside the ones covered by the spider."
msgstr ""

#: ../../faq.rst:221
msgid ""
"For more info see: "
":class:`~scrapy.spidermiddlewares.offsite.OffsiteMiddleware`."
msgstr ""

#: ../../faq.rst:225
msgid "What is the recommended way to deploy a Scrapy crawler in production?"
msgstr ""

#: ../../faq.rst:227
msgid "See :ref:`topics-deploy`."
msgstr ""

#: ../../faq.rst:230
msgid "Can I use JSON for large exports?"
msgstr ""

#: ../../faq.rst:232
msgid ""
"It'll depend on how large your output is. See :ref:`this warning <json-"
"with-large-data>` in :class:`~scrapy.exporters.JsonItemExporter` "
"documentation."
msgstr ""

#: ../../faq.rst:237
msgid "Can I return (Twisted) deferreds from signal handlers?"
msgstr ""

#: ../../faq.rst:239
msgid ""
"Some signals support returning deferreds from their handlers, others "
"don't. See the :ref:`topics-signals-ref` to know which ones."
msgstr ""

#: ../../faq.rst:243
msgid "What does the response status code 999 means?"
msgstr ""

#: ../../faq.rst:245
msgid ""
"999 is a custom response status code used by Yahoo sites to throttle "
"requests. Try slowing down the crawling speed by using a download delay "
"of ``2`` (or higher) in your spider::"
msgstr ""

#: ../../faq.rst:257
msgid ""
"Or by setting a global download delay in your project with the "
":setting:`DOWNLOAD_DELAY` setting."
msgstr ""

#: ../../faq.rst:261
msgid "Can I call ``pdb.set_trace()`` from my spiders to debug them?"
msgstr ""

#: ../../faq.rst:263
msgid ""
"Yes, but you can also use the Scrapy shell which allows you to quickly "
"analyze (and even modify) the response being processed by your spider, "
"which is, quite often, more useful than plain old ``pdb.set_trace()``."
msgstr ""

#: ../../faq.rst:267
msgid "For more info see :ref:`topics-shell-inspect-response`."
msgstr ""

#: ../../faq.rst:270
msgid "Simplest way to dump all my scraped items into a JSON/CSV/XML file?"
msgstr ""

#: ../../faq.rst:272
msgid "To dump into a JSON file::"
msgstr ""

#: ../../faq.rst:276
msgid "To dump into a CSV file::"
msgstr ""

#: ../../faq.rst:280
msgid "To dump into a XML file::"
msgstr ""

#: ../../faq.rst:284
msgid "For more information see :ref:`topics-feed-exports`"
msgstr ""

#: ../../faq.rst:287
msgid "What's this huge cryptic ``__VIEWSTATE`` parameter used in some forms?"
msgstr ""

#: ../../faq.rst:289
msgid ""
"The ``__VIEWSTATE`` parameter is used in sites built with ASP.NET/VB.NET."
" For more info on how it works see `this page`_. Also, here's an `example"
" spider`_ which scrapes one of these sites."
msgstr ""

#: ../../faq.rst:297
msgid "What's the best way to parse big XML/CSV data feeds?"
msgstr ""

#: ../../faq.rst:299
msgid ""
"Parsing big feeds with XPath selectors can be problematic since they need"
" to build the DOM of the entire feed in memory, and this can be quite "
"slow and consume a lot of memory."
msgstr ""

#: ../../faq.rst:303
msgid ""
"In order to avoid parsing all the entire feed at once in memory, you can "
"use the functions ``xmliter`` and ``csviter`` from "
"``scrapy.utils.iterators`` module. In fact, this is what the feed spiders"
" (see :ref:`topics-spiders`) use under the cover."
msgstr ""

#: ../../faq.rst:309
msgid "Does Scrapy manage cookies automatically?"
msgstr ""

#: ../../faq.rst:311
msgid ""
"Yes, Scrapy receives and keeps track of cookies sent by servers, and "
"sends them back on subsequent requests, like any regular web browser "
"does."
msgstr ""

#: ../../faq.rst:314
msgid "For more info see :ref:`topics-request-response` and :ref:`cookies-mw`."
msgstr ""

#: ../../faq.rst:317
msgid "How can I see the cookies being sent and received from Scrapy?"
msgstr ""

#: ../../faq.rst:319
msgid "Enable the :setting:`COOKIES_DEBUG` setting."
msgstr ""

#: ../../faq.rst:322
msgid "How can I instruct a spider to stop itself?"
msgstr ""

#: ../../faq.rst:324
msgid ""
"Raise the :exc:`~scrapy.exceptions.CloseSpider` exception from a "
"callback. For more info see: :exc:`~scrapy.exceptions.CloseSpider`."
msgstr ""

#: ../../faq.rst:328
msgid "How can I prevent my Scrapy bot from getting banned?"
msgstr ""

#: ../../faq.rst:330
msgid "See :ref:`bans`."
msgstr ""

#: ../../faq.rst:333
msgid "Should I use spider arguments or settings to configure my spider?"
msgstr ""

#: ../../faq.rst:335
msgid ""
"Both :ref:`spider arguments <spiderargs>` and :ref:`settings <topics-"
"settings>` can be used to configure your spider. There is no strict rule "
"that mandates to use one or the other, but settings are more suited for "
"parameters that, once set, don't change much, while spider arguments are "
"meant to change more often, even on each spider run and sometimes are "
"required for the spider to run at all (for example, to set the start url "
"of a spider)."
msgstr ""

#: ../../faq.rst:342
msgid ""
"To illustrate with an example, assuming you have a spider that needs to "
"log into a site to scrape data, and you only want to scrape data from a "
"certain section of the site (which varies each time). In that case, the "
"credentials to log in would be settings, while the url of the section to "
"scrape would be a spider argument."
msgstr ""

#: ../../faq.rst:349
msgid "I'm scraping a XML document and my XPath selector doesn't return any items"
msgstr ""

#: ../../faq.rst:351
msgid "You may need to remove namespaces. See :ref:`removing-namespaces`."
msgstr ""

#: ../../faq.rst:356
msgid "How to split an item into multiple items in an item pipeline?"
msgstr ""

#: ../../faq.rst:358
msgid ""
":ref:`Item pipelines <topics-item-pipeline>` cannot yield multiple items "
"per input item. :ref:`Create a spider middleware <custom-spider-"
"middleware>` instead, and use its "
":meth:`~scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output` "
"method for this purpose. For example::"
msgstr ""

#: ../../faq.rst:378
msgid "Does Scrapy support IPv6 addresses?"
msgstr ""

#: ../../faq.rst:380
msgid ""
"Yes, by setting :setting:`DNS_RESOLVER` to "
"``scrapy.resolver.CachingHostnameResolver``. Note that by doing so, you "
"lose the ability to set a specific timeout for DNS requests (the value of"
" the :setting:`DNS_TIMEOUT` setting is ignored)."
msgstr ""

#: ../../faq.rst:388
msgid ""
"How to deal with ``<class 'ValueError'>: filedescriptor out of range in "
"select()`` exceptions?"
msgstr ""

#: ../../faq.rst:390
msgid ""
"This issue `has been reported`_ to appear when running broad crawls in "
"macOS, where the default Twisted reactor is "
":class:`twisted.internet.selectreactor.SelectReactor`. Switching to a "
"different reactor is possible by using the :setting:`TWISTED_REACTOR` "
"setting."
msgstr ""

#: ../../faq.rst:398
msgid "How can I cancel the download of a given response?"
msgstr ""

#: ../../faq.rst:400
msgid ""
"In some situations, it might be useful to stop the download of a certain "
"response. For instance, sometimes you can determine whether or not you "
"need the full contents of a response by inspecting its headers or the "
"first bytes of its body. In that case, you could save resources by "
"attaching a handler to the :class:`~scrapy.signals.bytes_received` or "
":class:`~scrapy.signals.headers_received` signals and raising a "
":exc:`~scrapy.exceptions.StopDownload` exception. Please refer to the "
":ref:`topics-stop-response-download` topic for additional information and"
" examples."
msgstr ""

