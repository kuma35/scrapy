# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008–2018, Scrapy developers
# This file is distributed under the same license as the Scrapy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
msgid ""
msgstr ""
"Project-Id-Version: Scrapy \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-10 01:26+0900\n"
"PO-Revision-Date: 2021-06-08 02:42+0900\n"
"Last-Translator: kuma35\n"
"Language-Team: Japanese\n"
"Language: ja_JP\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0\n"
"Generated-By: Babel 2.7.0\n"

#: ../../topics/spiders.rst:5
msgid "Spiders"
msgstr "スパイダー"

#: ../../topics/spiders.rst:7
msgid ""
"Spiders are classes which define how a certain site (or a group of sites) "
"will be scraped, including how to perform the crawl (i.e. follow links) and "
"how to extract structured data from their pages (i.e. scraping items). In "
"other words, Spiders are the place where you define the custom behaviour for "
"crawling and parsing pages for a particular site (or, in some cases, a group "
"of sites)."
msgstr ""
"スパイダーは、特定のサイト(またはサイトのグループ)のスクレイピング方法を定義"
"するクラスです。クロールの実行方法(リンクの追跡など)やページから構造化データ"
"を抽出する方法(アイテムのスクレイピングなど)を含みます。 つまり、スパイダー"
"は、特定のサイト(場合によってはサイトのグループ)のページをクロールおよび解析"
"するためのカスタム動作を定義する場所です。"

#: ../../topics/spiders.rst:13
msgid "For spiders, the scraping cycle goes through something like this:"
msgstr "スパイダーのためのスクレイピング・サイクルは以下の通りです:"

#: ../../topics/spiders.rst:15
msgid ""
"You start by generating the initial Requests to crawl the first URLs, and "
"specify a callback function to be called with the response downloaded from "
"those requests."
msgstr ""
"最初のリクエストを生成して最初のURLをクロールし、それらのリクエストからダウン"
"ロードされたレスポンスで呼び出されるコールバック関数を指定することから始めま"
"す。"

#: ../../topics/spiders.rst:19
msgid ""
"The first requests to perform are obtained by calling the :meth:`~scrapy."
"spiders.Spider.start_requests` method which (by default) generates :class:"
"`~scrapy.http.Request` for the URLs specified in the :attr:`~scrapy.spiders."
"Spider.start_urls` and the :attr:`~scrapy.spiders.Spider.parse` method as "
"callback function for the Requests."
msgstr ""
"実行する最初のリクエストは、(デフォルトで) :attr:`~scrapy.spiders.Spider."
"start_urls` で指定されたURLの :class:`~scrapy.http.Request` を生成する :meth:"
"`~scrapy.spiders.Spider.start_requests` メソッドと、リクエストのコールバック"
"関数として :attr:`~scrapy.spiders.Spider.parse` メソッドを呼び出すことによっ"
"て取得されます。"

#: ../../topics/spiders.rst:26
msgid ""
"In the callback function, you parse the response (web page) and return :ref:"
"`item objects <topics-items>`, :class:`~scrapy.http.Request` objects, or an "
"iterable of these objects. Those Requests will also contain a callback "
"(maybe the same) and will then be downloaded by Scrapy and then their "
"response handled by the specified callback."
msgstr "コールバック関数では、レスポンス (Web ページ) をパースし、 :ref:`アイテム・オブジェクト <topics-items>` または :class:`~scrapy.http.Request` オブジェクトまたはこれらのオブジェクトの反復可能オブジェクトイテラブル(iterable)を返します。 これらのリクエストにはコールバック(同じコールバックの場合もあります) も含まれ、Scrapy によってダウンロードされ、指定されたコールバックによってレスポンスが処理されます。"

#: ../../topics/spiders.rst:33
msgid ""
"In callback functions, you parse the page contents, typically using :ref:"
"`topics-selectors` (but you can also use BeautifulSoup, lxml or whatever "
"mechanism you prefer) and generate items with the parsed data."
msgstr ""
"コールバック関数内では、通常 :ref:`topics-selectors` を使用してページ内容を"
"パースし、パースしたデータでアイテムを生成します(しかし、パースには、"
"BeautifulSoup、lxml、または任意のメカニズムを使用することもできます)。"

#: ../../topics/spiders.rst:37
msgid ""
"Finally, the items returned from the spider will be typically persisted to a "
"database (in some :ref:`Item Pipeline <topics-item-pipeline>`) or written to "
"a file using :ref:`topics-feed-exports`."
msgstr ""
"最後に、スパイダーから返されたアイテムは通常、データベースに保存されます(:"
"ref:`アイテム パイプライン<topics-item-pipeline>` を使う事もあります)また"
"は  :ref:`topics-feed-exports` を使用してファイルに書き込まれます。"

#: ../../topics/spiders.rst:41
msgid ""
"Even though this cycle applies (more or less) to any kind of spider, there "
"are different kinds of default spiders bundled into Scrapy for different "
"purposes. We will talk about those types here."
msgstr ""
"このサイクルはあらゆる種類のスパイダーに適用されます。そして更に、さまざまな"
"目的の為のさまざまな種類のデフォルト・スパイダーがScrapyに同梱されています。"
"以降、これらについても説明します。"

#: ../../topics/spiders.rst:51
msgid "scrapy.Spider"
msgstr "scrapy.Spider"

#: ../../topics/spiders.rst:55
msgid ""
"This is the simplest spider, and the one from which every other spider must "
"inherit (including spiders that come bundled with Scrapy, as well as spiders "
"that you write yourself). It doesn't provide any special functionality. It "
"just provides a default :meth:`start_requests` implementation which sends "
"requests from the :attr:`start_urls` spider attribute and calls the spider's "
"method ``parse`` for each of the resulting responses."
msgstr ""
"これは最も単純なスパイダーであり、他のすべてのスパイダーの継承元となるもので"
"す(Scrapyにバンドルされているスパイダーや、自分で作成したスパイダーを含む)。"
"特別な機能は提供しません。 :attr:`start_urls` スパイダー属性からリクエストを"
"送信し、結果の各レスポンスに対してスパイダーのメソッド ``parse`` を呼び出すデ"
"フォルトの :meth:`start_requests` 実装を提供するだけです。"

#: ../../topics/spiders.rst:64
msgid ""
"A string which defines the name for this spider. The spider name is how the "
"spider is located (and instantiated) by Scrapy, so it must be unique. "
"However, nothing prevents you from instantiating more than one instance of "
"the same spider. This is the most important spider attribute and it's "
"required."
msgstr ""
"このスパイダーの名前を定義する文字列。 スパイダー名は、スパイダーがScrapyに"
"よってどのように配置(およびインスタンス化)されるかであるため、一意でなければ"
"なりません。 ただし、同じスパイダーの複数のインスタンスをインスタンス化するこ"
"とを妨げるものはありません。 これは最も重要なスパイダーの属性であり、必須で"
"す。"

#: ../../topics/spiders.rst:70
msgid ""
"If the spider scrapes a single domain, a common practice is to name the "
"spider after the domain, with or without the `TLD`_. So, for example, a "
"spider that crawls ``mywebsite.com`` would often be called ``mywebsite``."
msgstr ""
"スパイダーが単一のドメインをスクレイピングする場合、一般的な方法は、`TLD`_ の"
"有無にかかわらず、ドメインに基づいてスパイダーに名前を付けることです。よっ"
"て、たとえば、 ``mywebsite.com`` をクロールするスパイダーは、しばしば "
"``mywebsite`` と呼ばれます。"

#: ../../topics/spiders.rst:77
msgid ""
"An optional list of strings containing domains that this spider is allowed "
"to crawl. Requests for URLs not belonging to the domain names specified in "
"this list (or their subdomains) won't be followed if :class:`~scrapy."
"spidermiddlewares.offsite.OffsiteMiddleware` is enabled."
msgstr ""
"このスパイダーがクロールできるドメインを含む文字列のオプションのリスト。 :"
"class:`~scrapy.spidermiddlewares.offsite.OffsiteMiddleware` が有効になってい"
"る場合、このリスト(またはそのサブドメイン)で指定されたドメイン名に属さないURL"
"のリクエストは追跡されません。"

#: ../../topics/spiders.rst:82
msgid ""
"Let's say your target url is ``https://www.example.com/1.html``, then add "
"``'example.com'`` to the list."
msgstr ""
"あなたのターゲットURLが ``https://www.example.com/1.html`` である場合、リスト"
"に ``'example.com'`` を追加します。"

#: ../../topics/spiders.rst:87
msgid ""
"A list of URLs where the spider will begin to crawl from, when no particular "
"URLs are specified. So, the first pages downloaded will be those listed "
"here. The subsequent :class:`~scrapy.http.Request` will be generated "
"successively from data contained in the start URLs."
msgstr ""
"特定のURLが指定されていない場合に、スパイダーがクロールを開始するURLのリス"
"ト。 したがって、ダウンロードされる最初のページはここにリストされているページ"
"になります。 後続の :class:`~scrapy.http.Request` は、開始URLに含まれるデータ"
"から連続して生成されます。"

#: ../../topics/spiders.rst:94
msgid ""
"A dictionary of settings that will be overridden from the project wide "
"configuration when running this spider. It must be defined as a class "
"attribute since the settings are updated before instantiation."
msgstr ""
"このスパイダーを実行するときにプロジェクト全体の設定から上書きされる設定の辞"
"書。 インスタンス化の前に設定が更新されるため、クラス属性として定義する必要が"
"あります。"

#: ../../topics/spiders.rst:98
msgid ""
"For a list of available built-in settings see: :ref:`topics-settings-ref`."
msgstr ""
"利用可能な組み込み設定のリストについては、 :ref:`topics-settings-ref` を参照"
"してください。"

#: ../../topics/spiders.rst:103
msgid ""
"This attribute is set by the :meth:`from_crawler` class method after "
"initializating the class, and links to the :class:`~scrapy.crawler.Crawler` "
"object to which this spider instance is bound."
msgstr ""
"この属性は、クラスを初期化した後に :meth:`from_crawler` クラスメソッドによっ"
"て設定され、このスパイダーインスタンスがバインドされている :class:`~scrapy."
"crawler.Crawler` オブジェクトにリンクします。"

#: ../../topics/spiders.rst:108
msgid ""
"Crawlers encapsulate a lot of components in the project for their single "
"entry access (such as extensions, middlewares, signals managers, etc). See :"
"ref:`topics-api-crawler` to know more about them."
msgstr ""
"クローラーは、単一のエントリアクセス(拡張機能、ミドルウェア、シグナルマネー"
"ジャーなど)のために、プロジェクト内の多くのコンポーネントをカプセル化しま"
"す。 :ref:`topics-api-crawler` を参照して、それらの詳細を確認してください。"

#: ../../topics/spiders.rst:114
msgid ""
"Configuration for running this spider. This is a :class:`~scrapy.settings."
"Settings` instance, see the :ref:`topics-settings` topic for a detailed "
"introduction on this subject."
msgstr ""
"このスパイダーを実行するための構成(Configuration)。 これは :class:`~scrapy."
"settings.Settings` のインスタンスです。この主題の詳細な紹介については :ref:"
"`topics-settings` トピックを参照してください。"

#: ../../topics/spiders.rst:120
msgid ""
"Python logger created with the Spider's :attr:`name`. You can use it to send "
"log messages through it as described on :ref:`topics-logging-from-spiders`."
msgstr ""
"Spiderの :attr:`name` で作成されたPythonロガー。 :ref:`topics-logging-from-"
"spiders` で説明されているように、これを使用してログメッセージを送信できます。"

#: ../../topics/spiders.rst:126
msgid "This is the class method used by Scrapy to create your spiders."
msgstr "これは、Scrapyがスパイダーを作成するために使用するクラスメソッドです。"

#: ../../topics/spiders.rst:128
msgid ""
"You probably won't need to override this directly because the default "
"implementation acts as a proxy to the :meth:`__init__` method, calling it "
"with the given arguments ``args`` and named arguments ``kwargs``."
msgstr ""
"デフォルトの実装は :meth:`__init__`  メソッドのプロキシとして機能し、指定され"
"た引数 ``args`` および名前付き引数 ``kwargs`` で呼び出すため、おそらくあなた"
"がこれを直接オーバーライドする必要はありません。"

#: ../../topics/spiders.rst:132
msgid ""
"Nonetheless, this method sets the :attr:`crawler` and :attr:`settings` "
"attributes in the new instance so they can be accessed later inside the "
"spider's code."
msgstr ""
"それにもかかわらず、このメソッドは新しいインスタンスで :attr:`crawler` およ"
"び :attr:`settings` 属性を設定するため、スパイダーのコード内で後からアクセス"
"できます。"

#: ../../topics/spiders.rst:0
msgid "Parameters"
msgstr "パラメータ"

#: ../../topics/spiders.rst:136
msgid "crawler to which the spider will be bound"
msgstr "スパイダーをバインドするクローラー"

#: ../../topics/spiders.rst:139
msgid "arguments passed to the :meth:`__init__` method"
msgstr ":meth:`__init__` メソッドに渡される引数"

#: ../../topics/spiders.rst:142
msgid "keyword arguments passed to the :meth:`__init__` method"
msgstr ":meth:`__init__` メソッドに渡されるキーワード引数"

#: ../../topics/spiders.rst:147
msgid ""
"This method must return an iterable with the first Requests to crawl for "
"this spider. It is called by Scrapy when the spider is opened for scraping. "
"Scrapy calls it only once, so it is safe to implement :meth:`start_requests` "
"as a generator."
msgstr ""
"このメソッドは、このスパイダーの最初のクロール要求で反復可能オブジェクト"
"(iterable)を返す必要があります。 スパイダーがスクレイピングのために開かれる"
"と、Scrapyによって呼び出されます。 Scrapyはこれを1回だけ呼び出すため、ジェネ"
"レータとして :meth:`start_requests` を実装しても安全です。"

#: ../../topics/spiders.rst:152
msgid ""
"The default implementation generates ``Request(url, dont_filter=True)`` for "
"each url in :attr:`start_urls`."
msgstr ""
"デフォルトの実装は、 :attr:`start_urls` の各URLに対して ``Request(url, "
"dont_filter=True)`` を生成します。"

#: ../../topics/spiders.rst:155
msgid ""
"If you want to change the Requests used to start scraping a domain, this is "
"the method to override. For example, if you need to start by logging in "
"using a POST request, you could do::"
msgstr ""
"ドメインのスクレイピングを開始するために使用されるリクエストを変更する場合、"
"これはオーバーライドするメソッドです。 たとえば、POST要求を使用してログインす"
"ることから開始する必要がある場合は、以下の通りです::"

#: ../../topics/spiders.rst:174
msgid ""
"This is the default callback used by Scrapy to process downloaded responses, "
"when their requests don't specify a callback."
msgstr ""
"これは、リクエストでコールバックが指定されていない場合に、ダウンロードされた"
"レスポンスを処理するためにScrapyが使用するデフォルトのコールバックです。"

#: ../../topics/spiders.rst:177
msgid ""
"The ``parse`` method is in charge of processing the response and returning "
"scraped data and/or more URLs to follow. Other Requests callbacks have the "
"same requirements as the :class:`Spider` class."
msgstr ""
"``parse`` メソッドは、レスポンスを処理し、スクレイピングされたデータや後続の"
"URLを返します。 他のリクエストのコールバックには、 :class:`Spider` クラスと同"
"じ必要条件があります。"

#: ../../topics/spiders.rst:181
msgid ""
"This method, as well as any other Request callback, must return an iterable "
"of :class:`~scrapy.http.Request` and/or :ref:`item objects <topics-items>`."
msgstr "このメソッドは、他の Request コールバックと同様に、 :class:`~scrapy.http.Request` および/または :ref:`アイテム・オブジェクト <topics-items>` の反復可能オブジェクト(iterable)を返さなければなりません。"

#: ../../topics/spiders.rst:185
msgid "the response to parse"
msgstr "パース対象のレスポンス"

#: ../../topics/spiders.rst:190
msgid ""
"Wrapper that sends a log message through the Spider's :attr:`logger`, kept "
"for backward compatibility. For more information see :ref:`topics-logging-"
"from-spiders`."
msgstr ""
"Spiderの :attr:`logger` を介してログメッセージを送信するラッパー。後方互換性"
"のために保持されています。 詳細については、 :ref:`topics-logging-from-"
"spiders` を参照してください。"

#: ../../topics/spiders.rst:196
msgid ""
"Called when the spider closes. This method provides a shortcut to signals."
"connect() for the :signal:`spider_closed` signal."
msgstr ""
"スパイダーが閉じるときに呼び出されます。 このメソッドは、 :signal:"
"`spider_closed` シグナルの signals.connect() へのショートカットを提供します。"

#: ../../topics/spiders.rst:199
msgid "Let's see an example::"
msgstr "ある例を見てみましょう::"

#: ../../topics/spiders.rst:216
msgid "Return multiple Requests and items from a single callback::"
msgstr "単一のコールバックから複数のリクエストとアイテムを返します::"

#: ../../topics/spiders.rst:236
msgid ""
"Instead of :attr:`~.start_urls` you can use :meth:`~.start_requests` "
"directly; to give data more structure you can use :class:`~scrapy.item.Item` "
"objects::"
msgstr ":attr:`~.start_urls` の代わりに :meth:`~.start_requests` を直接使用できます。データをさらに構造化するには、 :class:`~scrapy.item.Item` オブジェクトを使用できます::"

#: ../../topics/spiders.rst:261
msgid "Spider arguments"
msgstr "スパイダー引数"

#: ../../topics/spiders.rst:263
msgid ""
"Spiders can receive arguments that modify their behaviour. Some common uses "
"for spider arguments are to define the start URLs or to restrict the crawl "
"to certain sections of the site, but they can be used to configure any "
"functionality of the spider."
msgstr ""
"スパイダーは、振る舞いを変更する引数を受け取ることができます。 スパイダー引数"
"の一般的な使用法のいくつかは、開始URLを定義するか、サイトの特定のセクションへ"
"のクロールを制限することですが、スパイダーの機能を構成(configure)するためでも"
"使用できます。"

#: ../../topics/spiders.rst:268
msgid ""
"Spider arguments are passed through the :command:`crawl` command using the "
"``-a`` option. For example::"
msgstr ""
"スパイダー引数は、 :command:`crawl` コマンドの ``-a`` コマンドライン・オプ"
"ションを使用して渡します。"

#: ../../topics/spiders.rst:273
msgid "Spiders can access arguments in their `__init__` methods::"
msgstr "スパイダーは、 `__init__` メソッド内の引数にアクセスできます。::"

#: ../../topics/spiders.rst:285
msgid ""
"The default `__init__` method will take any spider arguments and copy them "
"to the spider as attributes. The above example can also be written as "
"follows::"
msgstr ""
"デフォルトの `__init__` メソッドはスパイダー引数を取り、それらを属性としてス"
"パイダーにコピーします。 上記の例は次のように書くこともできます::"

#: ../../topics/spiders.rst:297
msgid ""
"Keep in mind that spider arguments are only strings. The spider will not do "
"any parsing on its own. If you were to set the ``start_urls`` attribute from "
"the command line, you would have to parse it on your own into a list using "
"something like :func:`ast.literal_eval` or :func:`json.loads` and then set "
"it as an attribute. Otherwise, you would cause iteration over a "
"``start_urls`` string (a very common python pitfall) resulting in each "
"character being seen as a separate url."
msgstr "スパイダー引数は文字列にすぎないことに注意してください。 スパイダー自身はスパイダー引数文字列の解析を行いません。 コマンドラインから ``start_urls`` 属性を設定する場合、 :func:`ast.literal_eval` または :func:`json.loads` のようなものを使用して自分でリストに落とし込み、それを属性として設定する必要があります。そうしないと、``start_urls`` 文字列を反復して、各文字が個別のURLとして認識されることになります(訳注:pythonによくある落とし穴で、``list('hoge')`` は ``['h','o','g','e']`` になる)。"

#: ../../topics/spiders.rst:307
msgid ""
"A valid use case is to set the http auth credentials used by :class:`~scrapy."
"downloadermiddlewares.httpauth.HttpAuthMiddleware` or the user agent used "
"by :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`::"
msgstr ""
"有効なユースケースは、 http認証資格情報(auth credentials)に使用される :class:"
"`~scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware` またはユーザエー"
"ジェントとして使用される :class:`~scrapy.downloadermiddlewares.useragent."
"UserAgentMiddleware` を設定することです。"

#: ../../topics/spiders.rst:314
msgid ""
"Spider arguments can also be passed through the Scrapyd ``schedule.json`` "
"API. See `Scrapyd documentation`_."
msgstr ""
"スパイダー引数は、Scrapyd ``schedule.json`` APIを介して渡すこともできます。 "
"`Scrapyd documentation`_ をご覧ください。"

#: ../../topics/spiders.rst:320
msgid "Generic Spiders"
msgstr "汎用スパイダー"

#: ../../topics/spiders.rst:322
msgid ""
"Scrapy comes with some useful generic spiders that you can use to subclass "
"your spiders from. Their aim is to provide convenient functionality for a "
"few common scraping cases, like following all links on a site based on "
"certain rules, crawling from `Sitemaps`_, or parsing an XML/CSV feed."
msgstr "Scrapyには、スパイダーのサブクラス化に使用できる便利な汎用スパイダーがいくつか付属しています。 それらの目的は、特定のルールに基づいてサイト上のすべてのリンクをたどったり、 `Sitemaps`_ からクロールしたり、XML/CSVフィードを解析するなど、いくつかの一般的なスクレイピング・パターンに便利な機能を提供することです。"

#: ../../topics/spiders.rst:327
msgid ""
"For the examples used in the following spiders, we'll assume you have a "
"project with a ``TestItem`` declared in a ``myproject.items`` module::"
msgstr ""
"※この節のスパイダー例は、 ``myproject.items`` モジュールで宣言された "
"``TestItem`` を含むプロジェクトがあると仮定しています::"

#: ../../topics/spiders.rst:341
msgid "CrawlSpider"
msgstr "CrawlSpider"

#: ../../topics/spiders.rst:345
msgid ""
"This is the most commonly used spider for crawling regular websites, as it "
"provides a convenient mechanism for following links by defining a set of "
"rules. It may not be the best suited for your particular web sites or "
"project, but it's generic enough for several cases, so you can start from it "
"and override it as needed for more custom functionality, or just implement "
"your own spider."
msgstr ""
"これは、一連のルールを定義してリンクをたどる便利なメカニズムを提供するため、"
"通常のWebサイトをクロールするために最も一般的に使用されるスパイダーです。 特"
"定のWebサイトやプロジェクトには最適ではないかもしれませんが、いくつかのケース"
"では十分に汎用的であるため、このスパイダーから始めて、必要に応じてカスタム機"
"能をオーバーライドしたり、独自のスパイダーを実装したりできます。"

#: ../../topics/spiders.rst:351
msgid ""
"Apart from the attributes inherited from Spider (that you must specify), "
"this class supports a new attribute:"
msgstr ""
"Spiderから継承された(指定必須の)属性以外に、このclassは新しい属性をサポートし"
"ます。:"

#: ../../topics/spiders.rst:356
msgid ""
"Which is a list of one (or more) :class:`Rule` objects.  Each :class:`Rule` "
"defines a certain behaviour for crawling the site. Rules objects are "
"described below. If multiple rules match the same link, the first one will "
"be used, according to the order they're defined in this attribute."
msgstr ""
"これは、1つ(または複数)の :class:`Rule` オブジェクトのリストです。 各 :class:"
"`Rule` サイトをクロールするための特定の動作を定義します。 規則オブジェクトに"
"ついては以下で説明します。 複数の規則が同じリンクに一致する場合、この属性で定"
"義されている順序に従って、一致する最初の規則が使用されます。"

#: ../../topics/spiders.rst:361
msgid "This spider also exposes an overrideable method:"
msgstr "このスパイダーにはオーバーライド可能なメソッドもあります:"

#: ../../topics/spiders.rst:365
msgid ""
"This method is called for each response produced for the URLs in the "
"spider's ``start_urls`` attribute. It allows to parse the initial responses "
"and must return either an :ref:`item object <topics-items>`, a :class:"
"`~scrapy.http.Request` object, or an iterable containing any of them."
msgstr "このメソッドは、スパイダーの ``start_urls`` 属性の URL に対して生成されたレスポンスごとに呼び出されます。初期レスポンス群のパースを可能にし、 :ref:`アイテム・オブジェクト <topics-items>` または :class:`~scrapy.http.Request` オブジェクトまたはそれらのいずれかを含む反復可能オブジェクト(iterable)を返さなければなりません。"

#: ../../topics/spiders.rst:372
msgid "Crawling rules"
msgstr "クロール規則"

#: ../../topics/spiders.rst:376
msgid ""
"``link_extractor`` is a :ref:`Link Extractor <topics-link-extractors>` "
"object which defines how links will be extracted from each crawled page. "
"Each produced link will be used to generate a :class:`~scrapy.http.Request` "
"object, which will contain the link's text in its ``meta`` dictionary (under "
"the ``link_text`` key). If omitted, a default link extractor created with no "
"arguments will be used, resulting in all links being extracted."
msgstr "``link_extractor`` は、クロールされた各ページからリンクを抽出する方法を定義する :ref:`リンク抽出<topics-link-extractors>` オブジェクトです。生成された各リンクは、 :class:`~scrapy.http.Request` オブジェクトを生成するために使用されます。このオブジェクトでは、``meta`` 辞書(``link_text`` キー)にリンクのテキストを含みます。省略した場合、引数なしで作成されたデフォルトのリンク抽出器が使用され、すべてのリンクを抽出します。"

#: ../../topics/spiders.rst:383
msgid ""
"``callback`` is a callable or a string (in which case a method from the "
"spider object with that name will be used) to be called for each link "
"extracted with the specified link extractor. This callback receives a :class:"
"`~scrapy.http.Response` as its first argument and must return either a "
"single instance or an iterable of :ref:`item objects <topics-items>` and/or :"
"class:`~scrapy.http.Request` objects (or any subclass of them). As mentioned "
"above, the received :class:`~scrapy.http.Response` object will contain the "
"text of the link that produced the :class:`~scrapy.http.Request` in its "
"``meta`` dictionary (under the ``link_text`` key)"
msgstr "``callback`` は、指定のリンク抽出器で抽出された各リンクに対して呼び出される呼び出し可能オブジェクト(callable)または文字列(この場合、その名前のスパイダー・オブジェクトのメソッドが使用されます)です。 このコールバックは :class:`~scrapy.http.Response` を最初の引数として受け取り、単一のインスタンスまたは :ref:`アイテム・オブジェクト <topics-items>` の反復可能オブジェクト(iterable) そして/または :class:`~scrapy.http.Request` オブジェクト(またはそのサブクラス)、のいずれかを返す必要があります。上記のように、受け取った :class:`~scrapy.http.Response` オブジェクトには、その ``meta`` 辞書に :class:`~scrapy.http.Request` を生成したリンクのテキストを含みます(``link_text`` キー)。"

#: ../../topics/spiders.rst:392
msgid ""
"``cb_kwargs`` is a dict containing the keyword arguments to be passed to the "
"callback function."
msgstr ""
"``cb_kwargs`` は、コールバック関数に渡されるキーワード引数を含む辞書です。"

#: ../../topics/spiders.rst:395
msgid ""
"``follow`` is a boolean which specifies if links should be followed from "
"each response extracted with this rule. If ``callback`` is None ``follow`` "
"defaults to ``True``, otherwise it defaults to ``False``."
msgstr ""
"``follow`` は、このルールで抽出された各レスポンスからリンクをたどるかどうかを"
"指定するブール値です。 ``callback`` がNoneの場合、 ``follow`` のデフォルトは "
"``True`` になります。それ以外の場合、デフォルトは ``False`` になります。"

#: ../../topics/spiders.rst:399
msgid ""
"``process_links`` is a callable, or a string (in which case a method from "
"the spider object with that name will be used) which will be called for each "
"list of links extracted from each response using the specified "
"``link_extractor``. This is mainly used for filtering purposes."
msgstr ""
"``process_links`` は呼び出し可能オブジェクト(callable)、または指定された "
"``link_extractor`` を使用して各レスポンスから抽出されたリンクのリストごとに呼"
"び出される文字列(この場合、その名前のスパイダー・オブジェクトのメソッドが使用"
"されます)です。これは主にフィルタリングの目的で使用されます。"

#: ../../topics/spiders.rst:404
msgid ""
"``process_request`` is a callable (or a string, in which case a method from "
"the spider object with that name will be used) which will be called for "
"every :class:`~scrapy.http.Request` extracted by this rule. This callable "
"should take said request as first argument and the :class:`~scrapy.http."
"Response` from which the request originated as second argument. It must "
"return a ``Request`` object or ``None`` (to filter out the request)."
msgstr ""
"``process_request`` は、この規則によって抽出されたすべての :class:`~scrapy."
"http.Request` に対して呼び出される呼び出し可能オブジェクト(callable)(または文"
"字列、その場合はその名前のスパイダー・オブジェクトのメソッドが使用されます)で"
"す。 この呼び出し可能オブジェクト(callable)は、最初の引数としてリクエストを受"
"け取り、2番目の引数としてリクエストの発信元である :class:`~scrapy.http."
"Response` を受け取る必要があります。 ``Request`` オブジェクト、または "
"``None`` を返す必要があります(リクエストを除外するため)。"

#: ../../topics/spiders.rst:411
msgid ""
"``errback`` is a callable or a string (in which case a method from the "
"spider object with that name will be used) to be called if any exception is "
"raised while processing a request generated by the rule. It receives a :"
"class:`Twisted Failure <twisted.python.failure.Failure>` instance as first "
"parameter."
msgstr "``errback`` は、ルールによって生成されたリクエストの処理中に例外が発生した場合に呼び出される呼び出し可能オブジェクトまたは文字列(この場合、その名前のスパイダーオブジェクトのメソッドが使用されます)です。 最初のパラメータとして :class:`Twisted Failure <twisted.python.failure.Failure>` インスタンスを受け取ります。"

#: ../../topics/spiders.rst:418
msgid ""
"Because of its internal implementation, you must explicitly set callbacks "
"for new requests when writing :class:`CrawlSpider`-based spiders; unexpected "
"behaviour can occur otherwise."
msgstr "内部実装のため、:class:`CrawlSpider` ベースのスパイダーを作成するときは、新しいリクエストのコールバックを明示的に設定する必要があります。 そうしないと、予期しない動作が発生する可能性があります。"

#: ../../topics/spiders.rst:422
msgid "The *errback* parameter."
msgstr "*errback* パラメータ。"

#: ../../topics/spiders.rst:426
msgid "CrawlSpider example"
msgstr "CrawlSpider例"

#: ../../topics/spiders.rst:428
msgid "Let's now take a look at an example CrawlSpider with rules::"
msgstr "では、規則を使用したCrawlSpiderの例を見てみましょう::"

#: ../../topics/spiders.rst:463
msgid ""
"This spider would start crawling example.com's home page, collecting "
"category links, and item links, parsing the latter with the ``parse_item`` "
"method. For each item response, some data will be extracted from the HTML "
"using XPath, and an :class:`~scrapy.item.Item` will be filled with it."
msgstr ""
"このスパイダーはexample.comのホームページのクロールを開始し、カテゴリ・リンク"
"とアイテム・リンクを収集し、後者を ``parse_item`` メソッドでパースします。 各"
"アイテムのレスポンスに対して、XPathを使用してHTMLからいくつかのデータを抽出"
"し、 :class:`~scrapy.item.Item` は抽出されたデータで満たされます。"

#: ../../topics/spiders.rst:469
msgid "XMLFeedSpider"
msgstr "XMLFeedSpider"

#: ../../topics/spiders.rst:473
msgid ""
"XMLFeedSpider is designed for parsing XML feeds by iterating through them by "
"a certain node name.  The iterator can be chosen from: ``iternodes``, "
"``xml``, and ``html``.  It's recommended to use the ``iternodes`` iterator "
"for performance reasons, since the ``xml`` and ``html`` iterators generate "
"the whole DOM at once in order to parse it.  However, using ``html`` as the "
"iterator may be useful when parsing XML with bad markup."
msgstr ""
"XMLFeedSpiderは、特定のノード名でXMLフィードを反復処理することにより、XML"
"フィードをパースするために設計されています。 イテレータは、「iternodes」、"
"「xml」、および「html」から選択できます。 ``xml`` および ``html`` イテレータ"
"はパースするために一度DOM全体を生成します。そのため、パフォーマンス上の理由か"
"ら ``iternodes`` イテレータを使用することをお勧めします。 ただし、不正なマー"
"クアップを使用したXMLを解析する場合は、イテレータとして ``html`` を使用すると"
"便利です。"

#: ../../topics/spiders.rst:480
msgid ""
"To set the iterator and the tag name, you must define the following class "
"attributes:"
msgstr ""
"イテレータとタグ名を設定するには、以下のクラス属性を定義する必要があります:"

#: ../../topics/spiders.rst:485
msgid "A string which defines the iterator to use. It can be either:"
msgstr "使用するイテレータを定義する文字列。 以下のいずれかです:"

#: ../../topics/spiders.rst:487
msgid "``'iternodes'`` - a fast iterator based on regular expressions"
msgstr "``'iternodes'`` - 正規表現に基づく高速イテレータ"

#: ../../topics/spiders.rst:489
msgid ""
"``'html'`` - an iterator which uses :class:`~scrapy.selector.Selector`. Keep "
"in mind this uses DOM parsing and must load all DOM in memory which could be "
"a problem for big feeds"
msgstr ""
"``'html'`` - :class:`~scrapy.selector.Selector` を使用するイテレータ。 これは"
"DOM解析を使用し、すべてのDOMをメモリにロードする必要があることに注意してくだ"
"さい。これは大きなフィードの場合に問題になる可能性があります。"

#: ../../topics/spiders.rst:493
msgid ""
"``'xml'`` - an iterator which uses :class:`~scrapy.selector.Selector`. Keep "
"in mind this uses DOM parsing and must load all DOM in memory which could be "
"a problem for big feeds"
msgstr ""
"``'xml'`` - :class:`~scrapy.selector.Selector` を使用するイテレータ。 これは"
"DOM解析を使用し、すべてのDOMをメモリにロードする必要があることに注意してくだ"
"さい。これは大きなフィードの場合に問題になる可能性があります。"

#: ../../topics/spiders.rst:497
msgid "It defaults to: ``'iternodes'``."
msgstr "デフォルトは ``'iternodes'`` です。"

#: ../../topics/spiders.rst:501
msgid ""
"A string with the name of the node (or element) to iterate in. Example::"
msgstr "反復するノード(または要素)の名前を表す文字列。例::"

#: ../../topics/spiders.rst:507
msgid ""
"A list of ``(prefix, uri)`` tuples which define the namespaces available in "
"that document that will be processed with this spider. The ``prefix`` and "
"``uri`` will be used to automatically register namespaces using the :meth:"
"`~scrapy.selector.Selector.register_namespace` method."
msgstr ""
"このスパイダーで処理されるドキュメントで利用可能な名前空間を定義する "
"``(prefix, uri)`` タプルのリスト。 ``prefix`` と ``uri`` は、 :meth:`~scrapy."
"selector.Selector.register_namespace` メソッドを使用して名前空間を自動的に登"
"録するために使用されます。"

#: ../../topics/spiders.rst:513
msgid ""
"You can then specify nodes with namespaces in the :attr:`itertag` attribute."
msgstr ""
"あなたは、それから、 :attr:`itertag` 属性に名前空間を持つノードを指定できま"
"す。"

#: ../../topics/spiders.rst:516
msgid "Example::"
msgstr "例::"

#: ../../topics/spiders.rst:524
msgid ""
"Apart from these new attributes, this spider has the following overrideable "
"methods too:"
msgstr ""
"これらの新しい属性とは別に、このスパイダーには以下のオーバーライド可能なメ"
"ソッドもあります。:"

#: ../../topics/spiders.rst:529
msgid ""
"A method that receives the response as soon as it arrives from the spider "
"middleware, before the spider starts parsing it. It can be used to modify "
"the response body before parsing it. This method receives a response and "
"also returns a response (it could be the same or another one)."
msgstr ""
"スパイダー・ミドルウェアから到着するとすぐに、スパイダーがパース開始する前"
"に、レスポンスを受信するメソッド。 パース前にレスポンス・ボディを変更するため"
"に使用できます。 このメソッドはレスポンスを受け取り、レスポンスを返します(同"
"じ、または別のレスポンスになる可能性があります)。"

#: ../../topics/spiders.rst:536
msgid ""
"This method is called for the nodes matching the provided tag name "
"(``itertag``).  Receives the response and an :class:`~scrapy.selector."
"Selector` for each node.  Overriding this method is mandatory. Otherwise, "
"you spider won't work.  This method must return an :ref:`item object <topics-"
"items>`, a :class:`~scrapy.http.Request` object, or an iterable containing "
"any of them."
msgstr "このメソッドは、指定されたタグ名(``itertag``)に一致するノードに対して呼び出されます。 各ノードのレスポンス :class:`~scrapy.selector.Selector` を受け取ります。 このメソッドのオーバーライドは必須です。 そうしないと、このスパイダーは動作しません。 このメソッドは、 :ref:`アイテム・オブジェクト <topics-items>` または、 :class:`~scrapy.http.Request` オブジェクト、またはそれらのいずれかを含む反復可能オブジェクト(iterable)のいずれかを返す必要があります。"

#: ../../topics/spiders.rst:546
msgid ""
"This method is called for each result (item or request) returned by the "
"spider, and it's intended to perform any last time processing required "
"before returning the results to the framework core, for example setting the "
"item IDs. It receives a list of results and the response which originated "
"those results. It must return a list of results (items or requests)."
msgstr ""
"このメソッドは、スパイダーによって返された各結果(アイテムまたはリクエスト)に"
"対して呼び出され、結果をフレームワーク・コアに返す前に必要な最後の処理(アイテ"
"ムIDの設定など)を実行することを目的としています。 結果のリストと、それらの結"
"果を生成したレスポンスを受け取ります。 結果(アイテムまたはリクエスト)のリスト"
"を返す必要があります。"

#: ../../topics/spiders.rst:553
msgid ""
"Because of its internal implementation, you must explicitly set callbacks "
"for new requests when writing :class:`XMLFeedSpider`-based spiders; "
"unexpected behaviour can occur otherwise."
msgstr "内部実装のため、:class:`XMLFeedSpider` ベースのスパイダーを作成するときは、新しいリクエストのコールバックを明示的に設定する必要があります。 そうしないと、予期しない動作が発生する可能性があります。"

#: ../../topics/spiders.rst:559
msgid "XMLFeedSpider example"
msgstr "XMLFeedSpiderの例"

#: ../../topics/spiders.rst:561
msgid ""
"These spiders are pretty easy to use, let's have a look at one example::"
msgstr "これらのスパイダーは非常に使いやすいので、例を見てみましょう::"

#: ../../topics/spiders.rst:582
msgid ""
"Basically what we did up there was to create a spider that downloads a feed "
"from the given ``start_urls``, and then iterates through each of its "
"``item`` tags, prints them out, and stores some random data in an :class:"
"`~scrapy.item.Item`."
msgstr ""
"私たちがここで行ったことは、基本的には、指定した ``start_urls`` からフィード"
"をダウンロードし、それぞれの ``item`` タグを反復処理し、それらを出力し、いく"
"つかのランダムなデータを :class:`~scrapy.item.Item` に保存するスパイダーを作"
"成することです。"

#: ../../topics/spiders.rst:587
msgid "CSVFeedSpider"
msgstr "CSVFeedSpider"

#: ../../topics/spiders.rst:591
msgid ""
"This spider is very similar to the XMLFeedSpider, except that it iterates "
"over rows, instead of nodes. The method that gets called in each iteration "
"is :meth:`parse_row`."
msgstr ""
"このスパイダーはXMLFeedSpiderに非常に似ていますが、ノードではなく行を反復処理"
"する点が異なります。 各反復で呼び出されるメソッドは :meth:`parse_row` です。"

#: ../../topics/spiders.rst:597
msgid ""
"A string with the separator character for each field in the CSV file "
"Defaults to ``','`` (comma)."
msgstr ""
"CSVファイルの各フィールドを区切る文字(文字列)。デフォルトは ``','`` (カン"
"マ)。"

#: ../../topics/spiders.rst:602
msgid ""
"A string with the enclosure character for each field in the CSV file "
"Defaults to ``'\"'`` (quotation mark)."
msgstr ""
"CSVファイルの各フィールドを囲い込む文字(文字列)。デフォルトは ``'\\\"'`` (ダ"
"ブルクォーテーション)。"

#: ../../topics/spiders.rst:607
msgid "A list of the column names in the CSV file."
msgstr "CSVファイルの列名のリスト。"

#: ../../topics/spiders.rst:611
msgid ""
"Receives a response and a dict (representing each row) with a key for each "
"provided (or detected) header of the CSV file.  This spider also gives the "
"opportunity to override ``adapt_response`` and ``process_results`` methods "
"for pre- and post-processing purposes."
msgstr ""
"CSVファイルの、レスポンスと、提供された(または検出された)ヘッダー行ごとにキー"
"を持つ、(各行を表す)辞書を受け取ります。 このスパイダーは、前処理および後処理"
"のために ``adapt_response`` および ``process_results`` メソッドをオーバーライ"
"ドする機会も与えます。"

#: ../../topics/spiders.rst:617
msgid "CSVFeedSpider example"
msgstr "CSVFeedSpider例"

#: ../../topics/spiders.rst:619
msgid ""
"Let's see an example similar to the previous one, but using a :class:"
"`CSVFeedSpider`::"
msgstr ""
"いささか前の例に似ているけれども、 :class:`CSVFeedSpider` を使用している例を"
"見てみましょう::"

#: ../../topics/spiders.rst:644
msgid "SitemapSpider"
msgstr "SitemapSpider"

#: ../../topics/spiders.rst:648
msgid ""
"SitemapSpider allows you to crawl a site by discovering the URLs using "
"`Sitemaps`_."
msgstr ""
"SitemapSpiderでは、 `Sitemaps`_ を使用してURLを検出することにより、サイトをク"
"ロールできます。"

#: ../../topics/spiders.rst:651
msgid ""
"It supports nested sitemaps and discovering sitemap urls from `robots.txt`_."
msgstr ""
"ネストされたサイトマップをサポートし、 `robots.txt`_ からサイトマップのURLを"
"検出します。"

#: ../../topics/spiders.rst:656
msgid "A list of urls pointing to the sitemaps whose urls you want to crawl."
msgstr "あなたがクロールしたいサイトマップのURLを指定するURLのリスト。"

#: ../../topics/spiders.rst:658
msgid ""
"You can also point to a `robots.txt`_ and it will be parsed to extract "
"sitemap urls from it."
msgstr ""
"また、 あなたは `robots.txt`_ を指定することもできます。robots.txtは、サイト"
"マップのURLをパースするために解析されます。"

#: ../../topics/spiders.rst:663
msgid "A list of tuples ``(regex, callback)`` where:"
msgstr "タプル ``(regex, callback)`` のリスト。その内訳は以下の通りです:"

#: ../../topics/spiders.rst:665
msgid ""
"``regex`` is a regular expression to match urls extracted from sitemaps. "
"``regex`` can be either a str or a compiled regex object."
msgstr ""
"``regex`` は、サイトマップから抽出するURLに一致する正規表現です。 ``regex`` "
"は文字列またはコンパイル済みの正規表現オブジェクトのいずれかです。"

#: ../../topics/spiders.rst:668
msgid ""
"callback is the callback to use for processing the urls that match the "
"regular expression. ``callback`` can be a string (indicating the name of a "
"spider method) or a callable."
msgstr ""
"``callback`` は、正規表現に一致するURLの処理に使用するコールバックです。 "
"``callback`` は文字列(スパイダーメソッドの名前を示す)または呼び出し可能オブ"
"ジェクト(callable)です。"

#: ../../topics/spiders.rst:672 ../../topics/spiders.rst:696
#: ../../topics/spiders.rst:714
msgid "For example::"
msgstr "例えば::"

#: ../../topics/spiders.rst:676
msgid ""
"Rules are applied in order, and only the first one that matches will be used."
msgstr "順番に規則の適用を試み、一致する最初の規則のみが使用されます。"

#: ../../topics/spiders.rst:679
msgid ""
"If you omit this attribute, all urls found in sitemaps will be processed "
"with the ``parse`` callback."
msgstr ""
"あなたがこの属性を省略すると、サイトマップで見つかったすべてのURLは "
"``parse`` コールバックで処理されます。"

#: ../../topics/spiders.rst:684
msgid ""
"A list of regexes of sitemap that should be followed. This is only for sites "
"that use `Sitemap index files`_ that point to other sitemap files."
msgstr ""
"追跡すべきサイトマップの正規表現のリスト。 これは、他のサイトマップファイルを"
"指す `Sitemap index files`_  を使用するサイト専用です。"

#: ../../topics/spiders.rst:688
msgid "By default, all sitemaps are followed."
msgstr "デフォルトでは、すべてのサイトマップが追跡されます。"

#: ../../topics/spiders.rst:692
msgid ""
"Specifies if alternate links for one ``url`` should be followed. These are "
"links for the same website in another language passed within the same "
"``url`` block."
msgstr ""
"ある ``url`` の代替リンクをたどるかどうかを指定します。 これらは、同じ "
"``url`` ブロック内で渡される別の言語の同じWebサイトへのリンクです。"

#: ../../topics/spiders.rst:703
msgid ""
"With ``sitemap_alternate_links`` set, this would retrieve both URLs. With "
"``sitemap_alternate_links`` disabled, only ``http://example.com/`` would be "
"retrieved."
msgstr ""
"``sitemap_alternate_links`` を設定すると、両方のURLが取得されます。 "
"``sitemap_alternate_links`` を無効にすると、 ``http://example.com/`` のみが取"
"得されます。"

#: ../../topics/spiders.rst:707
msgid "Default is ``sitemap_alternate_links`` disabled."
msgstr "デフォルトでは ``sitemap_alternate_links`` は無効です。"

#: ../../topics/spiders.rst:711
msgid ""
"This is a filter function that could be overridden to select sitemap entries "
"based on their attributes."
msgstr ""
"これは、属性に基づいてサイトマップ・エントリを選択するためにオーバーライドで"
"きるフィルター関数です。"

#: ../../topics/spiders.rst:721
msgid ""
"We can define a ``sitemap_filter`` function to filter ``entries`` by date::"
msgstr ""
"私たちは、日付で  ``entries`` をフィルタリングする ``sitemap_filter`` 関数を"
"定義できます::"

#: ../../topics/spiders.rst:737
msgid ""
"This would retrieve only ``entries`` modified on 2005 and the following "
"years."
msgstr "これにより、2005年以降に変更された ``entries`` のみが取得されます。"

#: ../../topics/spiders.rst:740
msgid ""
"Entries are dict objects extracted from the sitemap document. Usually, the "
"key is the tag name and the value is the text inside it."
msgstr ""
"エントリは、サイトマップ・ドキュメントから抽出された辞書オブジェクトです。 通"
"常、キーはタグ名で、値はその中のテキストです。"

#: ../../topics/spiders.rst:743
msgid "It's important to notice that:"
msgstr "**重要な注意** :"

#: ../../topics/spiders.rst:745
msgid ""
"as the loc attribute is required, entries without this tag are discarded"
msgstr "loc属性が必要なため、このタグのないエントリは破棄されます。"

#: ../../topics/spiders.rst:746
msgid ""
"alternate links are stored in a list with the key ``alternate`` (see "
"``sitemap_alternate_links``)"
msgstr ""
"代替リンクはキー ``alternate`` でリストに保存されます"
"(``sitemap_alternate_links`` 参照)"

#: ../../topics/spiders.rst:748
msgid ""
"namespaces are removed, so lxml tags named as ``{namespace}tagname`` become "
"only ``tagname``"
msgstr ""
"名前空間が削除されるため、 ``{namespace}tagname`` という名前のlxmlタグは "
"``tagname`` のみになります。"

#: ../../topics/spiders.rst:750
msgid ""
"If you omit this method, all entries found in sitemaps will be processed, "
"observing other attributes and their settings."
msgstr ""
"あなたがこのメソッドを省略すると、サイトマップで見つかったすべてのエントリが"
"処理され、他の属性とその設定を参照します。"

#: ../../topics/spiders.rst:755
msgid "SitemapSpider examples"
msgstr "SitemapSpider例"

#: ../../topics/spiders.rst:757
msgid ""
"Simplest example: process all urls discovered through sitemaps using the "
"``parse`` callback::"
msgstr ""
"最も単純な例: ``parse`` コールバックを使用して、サイトマップを通じて検出され"
"たすべてのURLを処理します::"

#: ../../topics/spiders.rst:768
msgid ""
"Process some urls with certain callback and other urls with a different "
"callback::"
msgstr ""
"特定のコールバックでいくつかのURLを処理し、別個のコールバックでその他のURLを"
"処理します::"

#: ../../topics/spiders.rst:786
msgid ""
"Follow sitemaps defined in the `robots.txt`_ file and only follow sitemaps "
"whose url contains ``/sitemap_shop``::"
msgstr ""
"`robots.txt`_ ファイルで定義されたサイトマップに従い、URLに ``/"
"sitemap_shop`` が含まれるサイトマップのみを追跡します::"

#: ../../topics/spiders.rst:801
msgid "Combine SitemapSpider with other sources of urls::"
msgstr "SitemapSpiderとurlsの他のソースを組み合わせます::"
