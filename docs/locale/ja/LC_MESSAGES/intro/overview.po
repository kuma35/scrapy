# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008–2018, Scrapy developers
# This file is distributed under the same license as the Scrapy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
msgid ""
msgstr ""
"Project-Id-Version: Scrapy \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-10 01:26+0900\n"
"PO-Revision-Date: 2021-04-22 18:56+0900\n"
"Last-Translator: kuma35\n"
"Language-Team: Japanese\n"
"Language: ja_JP\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../intro/overview.rst:5
msgid "Scrapy at a glance"
msgstr "Scrapyを３行で説明シル"

#: ../../intro/overview.rst:7
msgid ""
"Scrapy is an application framework for crawling web sites and extracting "
"structured data which can be used for a wide range of useful applications, "
"like data mining, information processing or historical archival."
msgstr ""
"Scrapyは、Webサイトをクロールし、構造化されたデータを抽出するためのアプリケー"
"ション・フレームワークです。データ・フレームワークは、データ・マイニング、情"
"報処理、履歴アーカイブなど、さまざまな有用なアプリケーションに使用できます。"

#: ../../intro/overview.rst:11
msgid ""
"Even though Scrapy was originally designed for `web scraping`_, it can also "
"be used to extract data using APIs (such as `Amazon Associates Web "
"Services`_) or as a general purpose web crawler."
msgstr ""
"Scrapyは元々「ウェブ・スクレイピング(`web scraping`_)」用に設計されていました"
"が、API(`Amazon Associates Web Services`_ など)を使用してデータを抽出したり、"
"汎用のWebクローラーとして使用することもできます。"

# spider=蜘蛛。webが蜘蛛の巣であることからのネーミングと思われる。
#: ../../intro/overview.rst:17
msgid "Walk-through of an example spider"
msgstr "スパイダー例概観"

# Bring to the table
# 直訳： テーブルの上に持ち込む
# 意味：　有益なものを提供する
# http://www.bizteria.net/gc-english?p=ns&a=dt&id=342_iaZCRx
#: ../../intro/overview.rst:19
msgid ""
"In order to show you what Scrapy brings to the table, we'll walk you through "
"an example of a Scrapy Spider using the simplest way to run a spider."
msgstr ""
"Scrapyの恩恵を示すために、最も簡単な方法でスパイダーを実行するScrapy Spiderの"
"例を紹介します。"

#: ../../intro/overview.rst:22
msgid ""
"Here's the code for a spider that scrapes famous quotes from website http://"
"quotes.toscrape.com, following the pagination::"
msgstr ""
"以下は、Webサイトhttp://quotes.toscrape.comから有名な引用をスクレイピングする"
"スパイダーのコードです::"

#: ../../intro/overview.rst:45
msgid ""
"Put this in a text file, name it to something like ``quotes_spider.py`` and "
"run the spider using the :command:`runspider` command::"
msgstr ""
"これをテキスト・ファイルに入れ、「quotes_spider.py」などの名前を付けて、 :"
"command:`runspider` コマンドを使用してスパイダーを実行します::"

#: ../../intro/overview.rst:50
msgid ""
"When this finishes you will have in the ``quotes.jl`` file a list of the "
"quotes in JSON Lines format, containing text and author, looking like this::"
msgstr "これが完了すると、 ``quotes.jl`` ファイルに、以下のようなテキストと著者を含むJSON形式の引用のリストができます::"

#: ../../intro/overview.rst:60
msgid "What just happened?"
msgstr "あ…ありのまま 今 起こった事を話すぜ！"

#: ../../intro/overview.rst:62
msgid ""
"When you ran the command ``scrapy runspider quotes_spider.py``, Scrapy "
"looked for a Spider definition inside it and ran it through its crawler "
"engine."
msgstr ""
"あなたがコマンド ``scrapy runspider quotes_spider.py`` を実行すると、Scrapyは"
"その内部でスパイダー定義を探し、そのクローラー・エンジンを介して実行しまし"
"た。"

#: ../../intro/overview.rst:65
msgid ""
"The crawl started by making requests to the URLs defined in the "
"``start_urls`` attribute (in this case, only the URL for quotes in *humor* "
"category) and called the default callback method ``parse``, passing the "
"response object as an argument. In the ``parse`` callback, we loop through "
"the quote elements using a CSS Selector, yield a Python dict with the "
"extracted quote text and author, look for a link to the next page and "
"schedule another request using the same ``parse`` method as callback."
msgstr ""
"クロールは ``start_urls`` 属性で定義されたURL群(この場合、 *humor* カテゴリの"
"quoteのURLのみ)にリクエストを行うことで開始し、デフォルトのコールバック・メ"
"ソッド ``parse`` を呼び出して、引数としてレスポンス・オブジェクトを渡しま"
"す。 ``parse`` コールバックでは、CSSセレクターを使用してquote要素をループし、"
"抽出されたquoteテキストと著者を含むPython辞書を生成(yield)し、次のページへの"
"リンクを探し、同一のコールバック・メソッド ``parse`` を使用して次のリクエスト"
"をスケジュールします。"

#: ../../intro/overview.rst:73
msgid ""
"Here you notice one of the main advantages about Scrapy: requests are :ref:"
"`scheduled and processed asynchronously <topics-architecture>`.  This means "
"that Scrapy doesn't need to wait for a request to be finished and processed, "
"it can send another request or do other things in the meantime. This also "
"means that other requests can keep going even if some request fails or an "
"error happens while handling it."
msgstr ""
"ここで、Scrapyの主な利点の1つに気付きます。リクエストの :ref:`スケジューリン"
"グと処理は非同期<topics-architecture>` です。つまり、Scrapyはリクエストが終了"
"して処理されるのを待つ必要がなく、その間に別のリクエストを送信したり、他のこ"
"とを実行したりできます。 これは、一部のリクエストが失敗したり、処理中にエラー"
"が発生した場合でも、他のリクエストが続行できることも意味します。"

# B&Lポライトネス理論
# 彼らのポライトネス理論とは、“polite”という一般用語とは異なり、「円滑な人間関係を確立・維持するための言語行動」（宇佐美2002）と定義される、語用論の枠組みの中での概念である。(wikipedia)
#: ../../intro/overview.rst:80
msgid ""
"While this enables you to do very fast crawls (sending multiple concurrent "
"requests at the same time, in a fault-tolerant way) Scrapy also gives you "
"control over the politeness of the crawl through :ref:`a few settings "
"<topics-settings-ref>`. You can do things like setting a download delay "
"between each request, limiting amount of concurrent requests per domain or "
"per IP, and even :ref:`using an auto-throttling extension <topics-"
"autothrottle>` that tries to figure out these automatically."
msgstr ""
"これにより、非常に高速なクロール(フォールトトレラント(訳注:その構成部品の一部"
"が故障しても正常に処理を続行するシステム)な方法で複数の同時要求を同時に送信)"
"が可能になるけれども、更にScrapyでは :ref:`いくつかの設定<topics-settings-"
"ref>` を通してクロールのポライトネス(訳注:円滑な人間関係を確立・維持するため"
"の言語行動)を制御することもできます。各リクエスト間にダウンロード遅延を設定し"
"たり、ドメインごとまたはIPごとの同時リクエストの量を制限したり、これらを自動"
"的に把握しようとする :ref:`自動スロットル拡張の使用<topics-autothrottle>` も"
"可能です。"

#: ../../intro/overview.rst:90
msgid ""
"This is using :ref:`feed exports <topics-feed-exports>` to generate the JSON "
"file, you can easily change the export format (XML or CSV, for example) or "
"the storage backend (FTP or `Amazon S3`_, for example).  You can also write "
"an :ref:`item pipeline <topics-item-pipeline>` to store the items in a "
"database."
msgstr ""
"これは :ref:`フィード・エクスポート<topics-feed-exports>` を使用してJSONファ"
"イルを生成します。エクスポート形式(例えばXMLやCSVなど)またはストレージ・バッ"
"クエンド(例えば(FTPや `Amazon S3`_ )。あなたは  :ref:`アイテム・パイプライン"
"<topics-item-pipeline>` を記述して、アイテムをデータベースに保存することもで"
"きます。"

#: ../../intro/overview.rst:99
msgid "What else?"
msgstr "他に何かある？"

#: ../../intro/overview.rst:101
msgid ""
"You've seen how to extract and store items from a website using Scrapy, but "
"this is just the surface. Scrapy provides a lot of powerful features for "
"making scraping easy and efficient, such as:"
msgstr ""
"Scrapyを使用してWebサイトからアイテムを抽出および保存する方法を見てきました"
"が、これはほんのさわりです。 Scrapyはスクレイピングを簡単かつ効率的にするため"
"の次のような強力な機能を多数提供します:"

#: ../../intro/overview.rst:105
msgid ""
"Built-in support for :ref:`selecting and extracting <topics-selectors>` data "
"from HTML/XML sources using extended CSS selectors and XPath expressions, "
"with helper methods to extract using regular expressions."
msgstr ""
"HTML/XMLソースからの、データ :ref:`選択と抽出<topics-selectors>` のための拡張"
"CSSセレクターとXPath式の使用と、正規表現を使用して抽出するヘルパー・メソッド"
"を組み込みでサポート。"

#: ../../intro/overview.rst:109
msgid ""
"An :ref:`interactive shell console <topics-shell>` (IPython aware) for "
"trying out the CSS and XPath expressions to scrape data, very useful when "
"writing or debugging your spiders."
msgstr ""
":ref:`対話シェル<topics-shell>` (IPython対応)は、CSSおよびXPath式を試してデー"
"タをスクレイピングするためのもので、スパイダーを作成またはデバッグするときに"
"非常に便利です。"

#: ../../intro/overview.rst:113
msgid ""
"Built-in support for :ref:`generating feed exports <topics-feed-exports>` in "
"multiple formats (JSON, CSV, XML) and storing them in multiple backends "
"(FTP, S3, local filesystem)"
msgstr ""
"複数の形式(JSON、CSV、XML)でのデータ生成と、複数のバックエンド・タイプ(FTP、"
"S3、ローカルファイルシステム)に保存するための :ref:`フィード・エクスポート生"
"成<topics-feed-exports>` を組み込みでサポート。"

#: ../../intro/overview.rst:117
msgid ""
"Robust encoding support and auto-detection, for dealing with foreign, non-"
"standard and broken encoding declarations."
msgstr ""
"不明な、非標準や壊れたエンコーディング宣言を処理するための、強力なエンコード"
"支援と自動検出。"

#: ../../intro/overview.rst:120
msgid ""
":ref:`Strong extensibility support <extending-scrapy>`, allowing you to plug "
"in your own functionality using :ref:`signals <topics-signals>` and a well-"
"defined API (middlewares, :ref:`extensions <topics-extensions>`, and :ref:"
"`pipelines <topics-item-pipeline>`)."
msgstr ""
":ref:`強力な拡張性サポート<extending-scrapy>` により、 :ref:`シグナル<topics-"
"signals>` と明確に定義されたAPI(ミドルウェアと :ref:`拡張機能<topics-"
"extensions>` と :ref:`パイプライン<topics-item-pipeline>`)を使用して、あなた"
"独自の機能をプラグインできます。"

#: ../../intro/overview.rst:125
msgid "Wide range of built-in extensions and middlewares for handling:"
msgstr "広範な組み込み拡張機能と処理用のミドルウェア:"

#: ../../intro/overview.rst:127
msgid "cookies and session handling"
msgstr "クッキーやセッションの取扱"

#: ../../intro/overview.rst:128
msgid "HTTP features like compression, authentication, caching"
msgstr "圧縮、認証、キャッシングなどのHTTP機能"

#: ../../intro/overview.rst:129
msgid "user-agent spoofing"
msgstr "ユーザ・エージェントのなりすまし(spoofing)"

#: ../../intro/overview.rst:130
msgid "robots.txt"
msgstr "robots.txt"

#: ../../intro/overview.rst:131
msgid "crawl depth restriction"
msgstr "クロールの深さの制限"

#: ../../intro/overview.rst:132
msgid "and more"
msgstr "などなど"

#: ../../intro/overview.rst:134
msgid ""
"A :ref:`Telnet console <topics-telnetconsole>` for hooking into a Python "
"console running inside your Scrapy process, to introspect and debug your "
"crawler"
msgstr ""
":ref:`Telnet<topics-telnetconsole>` は、クローラー内部の調査およびデバッグの"
"ために、あなたのScrapyプロセス内で実行されているPythonコンソールにフックしま"
"す。"

#: ../../intro/overview.rst:138
msgid ""
"Plus other goodies like reusable spiders to crawl sites from `Sitemaps`_ and "
"XML/CSV feeds, a media pipeline for :ref:`automatically downloading images "
"<topics-media-pipeline>` (or any other media) associated with the scraped "
"items, a caching DNS resolver, and much more!"
msgstr ""
"さらに、その他の便利な機能として、サイトマップ(`Sitemaps`_)およびXML/CSV"
"フィードからサイトをクロールするための再利用可能なスパイダーや、 スクレイプさ"
"れたアイテムに関連付けられている、 画像(または他の媒体)を自動ダウンロードする"
"ための :ref:`媒体パイプライン<topics-media-pipeline>` や、DNSリゾルバのキャッ"
"シングなど、その他多数あります！"

#: ../../intro/overview.rst:144
msgid "What's next?"
msgstr "さてお次は？"

#: ../../intro/overview.rst:146
msgid ""
"The next steps for you are to :ref:`install Scrapy <intro-install>`, :ref:"
"`follow through the tutorial <intro-tutorial>` to learn how to create a full-"
"blown Scrapy project and `join the community`_. Thanks for your interest!"
msgstr ""
"お次は、 :ref:`Scrapyインストール<intro-install>` を行い :ref:`チュートリアル"
"<intro-tutorial>` で本格的なScrapyプロジェクトを作成する方法を学び、Scrapyコ"
"ミュニティに参加(`join the community`_)します。あなたがScrapyに興味を持ってく"
"れてありがうございます！"
